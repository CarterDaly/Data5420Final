{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>SP500_Open</th>\n",
       "      <th>SP500_Close</th>\n",
       "      <th>Day_Before_Close</th>\n",
       "      <th>Two_Days_Later_Close</th>\n",
       "      <th>Week_Later_Close</th>\n",
       "      <th>SP500_MA_3</th>\n",
       "      <th>SP500_MA_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>recession now... or stagflation forever</td>\n",
       "      <td>2024-09-13 16:45:00</td>\n",
       "      <td>5603.339844</td>\n",
       "      <td>5626.020020</td>\n",
       "      <td>5595.759766</td>\n",
       "      <td>5626.020020</td>\n",
       "      <td>5713.640137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mortgage rates are dropping, but homes are not...</td>\n",
       "      <td>2024-09-13 16:40:00</td>\n",
       "      <td>5603.339844</td>\n",
       "      <td>5626.020020</td>\n",
       "      <td>5595.759766</td>\n",
       "      <td>5626.020020</td>\n",
       "      <td>5713.640137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hospitality stocks out-innovate challenges as ...</td>\n",
       "      <td>2024-09-13 16:34:47</td>\n",
       "      <td>5603.339844</td>\n",
       "      <td>5626.020020</td>\n",
       "      <td>5595.759766</td>\n",
       "      <td>5626.020020</td>\n",
       "      <td>5713.640137</td>\n",
       "      <td>5626.020020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ratings agency fitch says extended strike at b...</td>\n",
       "      <td>2024-09-13 16:30:33</td>\n",
       "      <td>5603.339844</td>\n",
       "      <td>5626.020020</td>\n",
       "      <td>5595.759766</td>\n",
       "      <td>5626.020020</td>\n",
       "      <td>5713.640137</td>\n",
       "      <td>5626.020020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google parent company in bear territory, down ...</td>\n",
       "      <td>2024-09-13 16:28:47</td>\n",
       "      <td>5603.339844</td>\n",
       "      <td>5626.020020</td>\n",
       "      <td>5595.759766</td>\n",
       "      <td>5626.020020</td>\n",
       "      <td>5713.640137</td>\n",
       "      <td>5626.020020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>ing gives update on climate action approach, a...</td>\n",
       "      <td>2024-09-19 06:00:00</td>\n",
       "      <td>5702.629883</td>\n",
       "      <td>5713.640137</td>\n",
       "      <td>5618.259766</td>\n",
       "      <td>5702.549805</td>\n",
       "      <td>5722.259766</td>\n",
       "      <td>5681.846680</td>\n",
       "      <td>5645.511300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>the central bank bonanza continues with the bo...</td>\n",
       "      <td>2024-09-19 05:46:38</td>\n",
       "      <td>5702.629883</td>\n",
       "      <td>5713.640137</td>\n",
       "      <td>5618.259766</td>\n",
       "      <td>5702.549805</td>\n",
       "      <td>5722.259766</td>\n",
       "      <td>5713.640137</td>\n",
       "      <td>5659.137068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>final result of onni bidco's voluntary recomme...</td>\n",
       "      <td>2024-09-19 05:35:00</td>\n",
       "      <td>5702.629883</td>\n",
       "      <td>5713.640137</td>\n",
       "      <td>5618.259766</td>\n",
       "      <td>5702.549805</td>\n",
       "      <td>5722.259766</td>\n",
       "      <td>5713.640137</td>\n",
       "      <td>5672.762835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>sampo plc’s share buybacks 18 september 2024</td>\n",
       "      <td>2024-09-19 05:30:00</td>\n",
       "      <td>5702.629883</td>\n",
       "      <td>5713.640137</td>\n",
       "      <td>5618.259766</td>\n",
       "      <td>5702.549805</td>\n",
       "      <td>5722.259766</td>\n",
       "      <td>5713.640137</td>\n",
       "      <td>5686.388602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>why arizona iced tea is betting on spiked beve...</td>\n",
       "      <td>2024-09-19 05:00:00</td>\n",
       "      <td>5702.629883</td>\n",
       "      <td>5713.640137</td>\n",
       "      <td>5618.259766</td>\n",
       "      <td>5702.549805</td>\n",
       "      <td>5722.259766</td>\n",
       "      <td>5713.640137</td>\n",
       "      <td>5700.014369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title              pubDate  \\\n",
       "0              recession now... or stagflation forever  2024-09-13 16:45:00   \n",
       "1    mortgage rates are dropping, but homes are not...  2024-09-13 16:40:00   \n",
       "2    hospitality stocks out-innovate challenges as ...  2024-09-13 16:34:47   \n",
       "3    ratings agency fitch says extended strike at b...  2024-09-13 16:30:33   \n",
       "4    google parent company in bear territory, down ...  2024-09-13 16:28:47   \n",
       "..                                                 ...                  ...   \n",
       "195  ing gives update on climate action approach, a...  2024-09-19 06:00:00   \n",
       "196  the central bank bonanza continues with the bo...  2024-09-19 05:46:38   \n",
       "197  final result of onni bidco's voluntary recomme...  2024-09-19 05:35:00   \n",
       "198       sampo plc’s share buybacks 18 september 2024  2024-09-19 05:30:00   \n",
       "199  why arizona iced tea is betting on spiked beve...  2024-09-19 05:00:00   \n",
       "\n",
       "      SP500_Open  SP500_Close  Day_Before_Close  Two_Days_Later_Close  \\\n",
       "0    5603.339844  5626.020020       5595.759766           5626.020020   \n",
       "1    5603.339844  5626.020020       5595.759766           5626.020020   \n",
       "2    5603.339844  5626.020020       5595.759766           5626.020020   \n",
       "3    5603.339844  5626.020020       5595.759766           5626.020020   \n",
       "4    5603.339844  5626.020020       5595.759766           5626.020020   \n",
       "..           ...          ...               ...                   ...   \n",
       "195  5702.629883  5713.640137       5618.259766           5702.549805   \n",
       "196  5702.629883  5713.640137       5618.259766           5702.549805   \n",
       "197  5702.629883  5713.640137       5618.259766           5702.549805   \n",
       "198  5702.629883  5713.640137       5618.259766           5702.549805   \n",
       "199  5702.629883  5713.640137       5618.259766           5702.549805   \n",
       "\n",
       "     Week_Later_Close   SP500_MA_3   SP500_MA_7  \n",
       "0         5713.640137          NaN          NaN  \n",
       "1         5713.640137          NaN          NaN  \n",
       "2         5713.640137  5626.020020          NaN  \n",
       "3         5713.640137  5626.020020          NaN  \n",
       "4         5713.640137  5626.020020          NaN  \n",
       "..                ...          ...          ...  \n",
       "195       5722.259766  5681.846680  5645.511300  \n",
       "196       5722.259766  5713.640137  5659.137068  \n",
       "197       5722.259766  5713.640137  5672.762835  \n",
       "198       5722.259766  5713.640137  5686.388602  \n",
       "199       5722.259766  5713.640137  5700.014369  \n",
       "\n",
       "[200 rows x 9 columns]"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('articles_with_sp500.csv')\n",
    "\n",
    "df.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Convert pubDate to datetime\n",
    "df['pubDate'] = pd.to_datetime(df['pubDate'])\n",
    "\n",
    "# Convert headlines to numerical representation using TF-IDF\n",
    "vectorizer = TfidfVectorizer(min_df=1, max_df=0.95)\n",
    "X_text = vectorizer.fit_transform(df['title'])\n",
    "\n",
    "# Scale SP500_Open by dividing it by 100\n",
    "X_open_scaled = df['SP500_Open'] / 8000\n",
    "\n",
    "# Combine the scaled SP500_Open with the TF-IDF matrix\n",
    "X_combined = np.hstack((X_open_scaled.values.reshape(-1, 1), X_text.toarray()))\n",
    "\n",
    "# Extract the numerical features that will be the output targets\n",
    "numerical_features = df[['SP500_Close', 'Two_Days_Later_Close']].values\n",
    "\n",
    "pub_dates = df['pubDate']\n",
    "\n",
    "# Define the target variables (numerical features)\n",
    "target = numerical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, pub_dates_train, pub_dates_test = train_test_split(\n",
    "    X_combined, target, pub_dates, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "y_train_scaled = scaler.fit_transform(y_train)\n",
    "y_test_scaled = scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketPredictionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MarketPredictionModel, self).__init__()\n",
    "        \n",
    "        # Define the 12 layers\n",
    "        self.fc1 = nn.Linear(input_dim, 8192)  # First layer\n",
    "        self.fc2 = nn.Linear(8192, 4096)\n",
    "        self.fc3 = nn.Linear(4096, 4096)\n",
    "        self.fc4 = nn.Linear(4096, 4096)\n",
    "        self.fc5 = nn.Linear(4096, 4096)\n",
    "        self.fc6 = nn.Linear(4096, 2048)\n",
    "        self.fc7 = nn.Linear(2048, 2048)\n",
    "        self.fc8 = nn.Linear(2048, 1024)\n",
    "        self.fc9 = nn.Linear(1024, 512)\n",
    "        self.fc10 = nn.Linear(512, 256)\n",
    "        self.fc11 = nn.Linear(256, 128)\n",
    "        self.fc12 = nn.Linear(128, output_dim)  # Final output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply ReLU activation to each layer\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = torch.relu(self.fc6(x))\n",
    "        x = torch.relu(self.fc7(x))\n",
    "        x = torch.relu(self.fc8(x))\n",
    "        x = torch.relu(self.fc9(x))\n",
    "        x = torch.relu(self.fc10(x))\n",
    "        x = torch.relu(self.fc11(x))\n",
    "        x = self.fc12(x)  # Output layer (no activation for regression)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = X_train.shape[1]  # Number of features (TF-IDF features)\n",
    "output_dim = y_train_scaled.shape[1]  # Number of target variables (numerical features)\n",
    "model = MarketPredictionModel(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3503, 7860)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_scaled, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/90], Loss: 0.9993\n",
      "Epoch [20/90], Loss: 1.0004\n",
      "Epoch [30/90], Loss: 0.7978\n",
      "Epoch [40/90], Loss: 0.5554\n",
      "Epoch [50/90], Loss: 0.1752\n",
      "Epoch [60/90], Loss: 0.1049\n",
      "Epoch [70/90], Loss: 0.0878\n",
      "Epoch [80/90], Loss: 0.0809\n",
      "Epoch [90/90], Loss: 0.0790\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "num_epochs = 90\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    optimizer.zero_grad()  # Zero the gradients\n",
    "    outputs = model(X_train_tensor)  # Forward pass\n",
    "    loss = criterion(outputs, y_train_tensor)  # Compute the loss\n",
    "    loss.backward()  # Backward pass\n",
    "    optimizer.step()  # Update the weights\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE (Overall): 10977.688344238555\n",
      "MSE for SP500_Close: 11161.927814785748\n",
      "MSE for Two_Days_Later_Close: 10793.448873691363\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get predictions from the model\n",
    "    predictions_scaled = model(X_test_tensor)\n",
    "    \n",
    "    # Convert predictions to numpy for inverse scaling\n",
    "    predictions_scaled_numpy = predictions_scaled.numpy()\n",
    "\n",
    "    # Inverse scale the predictions to the original scale\n",
    "    predictions_unscaled = scaler.inverse_transform(predictions_scaled_numpy)\n",
    "\n",
    "    # Calculate the overall Mean Squared Error (MSE)\n",
    "    mse_overall = mean_squared_error(y_test, predictions_unscaled)\n",
    "    print(f\"Test MSE (Overall): {mse_overall}\")\n",
    "\n",
    "    # Calculate Mean Squared Error (MSE) for each feature\n",
    "    feature_names = ['SP500_Close', 'Two_Days_Later_Close']\n",
    "\n",
    "    for feature_idx, feature_name in enumerate(feature_names):\n",
    "        mse_feature = mean_squared_error(y_test[:, feature_idx], predictions_unscaled[:, feature_idx])\n",
    "        print(f\"MSE for {feature_name}: {mse_feature}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0\n",
      "    SP500_Close - Predicted: 5780.58203125, Actual: 5813.669921875\n",
      "    Two_Days_Later_Close - Predicted: 5784.189453125, Actual: 5728.7998046875\n",
      "--------------------------------------------------\n",
      "Sample 1\n",
      "    SP500_Close - Predicted: 5735.3212890625, Actual: 5732.93017578125\n",
      "    Two_Days_Later_Close - Predicted: 5739.921875, Actual: 5745.3701171875\n",
      "--------------------------------------------------\n",
      "Sample 2\n",
      "    SP500_Close - Predicted: 5817.56640625, Actual: 5712.68994140625\n",
      "    Two_Days_Later_Close - Predicted: 5821.28369140625, Actual: 5929.0400390625\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Optionally, print sample-wise predictions and actual values for each feature\n",
    "for i in range(3):\n",
    "    print(f\"Sample {i}\")\n",
    "    for feature_idx, feature_name in enumerate(feature_names):\n",
    "        print(f\"    {feature_name} - Predicted: {predictions_unscaled[i, feature_idx]}, Actual: {y_test[i, feature_idx]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 samples with the largest errors:\n",
      "Sample 1 (Index 321) - Max Error: 290.57\n",
      "    SP500_Close - Predicted: 5901.9091796875, Actual: 5633.08984375\n",
      "    Two_Days_Later_Close - Predicted: 5908.83349609375, Actual: 5618.259765625\n",
      "--------------------------------------------------\n",
      "Sample 2 (Index 67) - Max Error: 288.56\n",
      "    SP500_Close - Predicted: 5899.88134765625, Actual: 5633.08984375\n",
      "    Two_Days_Later_Close - Predicted: 5906.82177734375, Actual: 5618.259765625\n",
      "--------------------------------------------------\n",
      "Sample 3 (Index 391) - Max Error: 285.53\n",
      "    SP500_Close - Predicted: 5699.84814453125, Actual: 5985.3798828125\n",
      "    Two_Days_Later_Close - Predicted: 5705.36181640625, Actual: 5870.6201171875\n",
      "--------------------------------------------------\n",
      "Sample 4 (Index 740) - Max Error: 278.41\n",
      "    SP500_Close - Predicted: 5706.9677734375, Actual: 5985.3798828125\n",
      "    Two_Days_Later_Close - Predicted: 5712.2978515625, Actual: 5870.6201171875\n",
      "--------------------------------------------------\n",
      "Sample 5 (Index 408) - Max Error: 275.61\n",
      "    SP500_Close - Predicted: 5998.078125, Actual: 5813.669921875\n",
      "    Two_Days_Later_Close - Predicted: 6004.4072265625, Actual: 5728.7998046875\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Calculate absolute errors for each feature\n",
    "abs_errors = np.abs(predictions_unscaled - y_test)\n",
    "\n",
    "# Step 2: Calculate the maximum absolute error for each sample (across all features)\n",
    "max_abs_errors = np.max(abs_errors, axis=1)\n",
    "\n",
    "# Step 3: Sort samples by the maximum absolute error\n",
    "sorted_indices = np.argsort(max_abs_errors)[::-1]  # Sort in descending order\n",
    "\n",
    "# Step 4: Display the top N samples with the largest errors\n",
    "top_n = 5  # Set the number of top samples you want to view\n",
    "print(f\"Top {top_n} samples with the largest errors:\")\n",
    "\n",
    "for i in range(top_n):\n",
    "    idx = sorted_indices[i]\n",
    "    print(f\"Sample {i+1} (Index {idx}) - Max Error: {max_abs_errors[idx]:.2f}\")\n",
    "    for feature_idx, feature_name in enumerate(['SP500_Close', 'Two_Days_Later_Close']):\n",
    "        print(f\"    {feature_name} - Predicted: {predictions_unscaled[idx, feature_idx]}, Actual: {y_test[idx, feature_idx]}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted vs Actual data has been exported to 'results/pred_vs_actual.csv'\n"
     ]
    }
   ],
   "source": [
    "# Rows generation section\n",
    "rows = []\n",
    "\n",
    "# List of feature names\n",
    "feature_names = [\"SP500_Close\", \"Two_Days_Later_Close\"]\n",
    "\n",
    "# Ensure pub_dates_test is aligned with X_test and y_test\n",
    "for i in range(len(predictions_unscaled)):\n",
    "    # Use pub_dates_test for alignment\n",
    "    row = {\"Sample\": i, \"pubDate\": pub_dates_test.iloc[i]}  # Add the corresponding pubDate\n",
    "    for feature_idx, feature_name in enumerate(feature_names):\n",
    "        # Add predicted, actual, and error values\n",
    "        row[f\"{feature_name}_Predicted\"] = predictions_unscaled[i, feature_idx]\n",
    "        row[f\"{feature_name}_Actual\"] = y_test[i, feature_idx]\n",
    "        row[f\"{feature_name}_Error\"] = abs(predictions_unscaled[i, feature_idx] - y_test[i, feature_idx])\n",
    "    rows.append(row)\n",
    "\n",
    "# Convert rows to a DataFrame\n",
    "df_pred_vs_actual = pd.DataFrame(rows)\n",
    "\n",
    "# Export to CSV\n",
    "csv_filename = \"results/pred_vs_actual.csv\"\n",
    "df_pred_vs_actual.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f\"Predicted vs Actual data has been exported to '{csv_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SP500_Close_Error - Mean Error: 84.16, Median Error: 69.12\n",
      "Two_Days_Later_Close_Error - Mean Error: 83.30, Median Error: 68.30\n"
     ]
    }
   ],
   "source": [
    "#Final Eval\n",
    "df_pred_vs_actual = pd.read_csv(\"results/pred_vs_actual.csv\")\n",
    "\n",
    "# Compute mean and median for each error column\n",
    "error_columns = [col for col in df_pred_vs_actual.columns if \"Error\" in col]\n",
    "error_stats = {}\n",
    "\n",
    "for error_column in error_columns:\n",
    "    mean_error = df_pred_vs_actual[error_column].mean()\n",
    "    median_error = df_pred_vs_actual[error_column].median()\n",
    "    error_stats[error_column] = {\"Mean\": mean_error, \"Median\": median_error}\n",
    "\n",
    "# Print out the error statistics\n",
    "for error_column, stats in error_stats.items():\n",
    "    print(f\"{error_column} - Mean Error: {stats['Mean']:.2f}, Median Error: {stats['Median']:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textfinalenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
